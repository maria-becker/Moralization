{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanzen-Suche\n",
    "Mit diesem Notebook können alle Instanzen aufgelistet werden, die bestimmte Bedingungen erfüllen. Die Bedingungen können frei gewählt werden. Sie bestehen aus einer Annotationskategorie und einer weiteren Bedingung. Folgende Suchparameter gibt es:\n",
    "+ nach Wörtern (Lexemen)\n",
    "+ Nach Wortarten oder Wortartenlisten (benutzt wird das [Stuttgart-Tübingen-Tagset](https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html) oder [Universal POS tags](https://universaldependencies.org/u/pos/))\n",
    "+ Nach Anzahl vergebener Kategorietags (z.B.: 3 Moralwerte)\n",
    "+ Nach bestimmten Annotationslabels (z.B.: Forderer)\n",
    "\n",
    "Bezüglich der Annotationskategorien stehen Folgende zur Verfügung:\n",
    "+ *obj_morals*: Alle Moralwerte, die keine subjektiven Ausdrücke sind\n",
    "+ *subj_morals*: Moralwerte, die subjektive Ausdrücke sind\n",
    "+ *all_morals*: Alle Moralwerte\n",
    "+ *protagonists*: Alle Protagonisten-Token\n",
    "+ *protagonists_doubles*: Alle Protagonisten-Rollen (Achtung: manche Protagonisten-Token werden dabei doppelt gezählt - nämlich, wenn sie mehrere Rollen haben)\n",
    "+ *com_functions*: Kommunikative Funktionen\n",
    "+ *expl_demands*: Explizite Forderungen\n",
    "+ *impl_demands*: Implizite Forderungen\n",
    "+ *all_demands*: Alle Forderungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muss ausgeführt werden, wenn das Notebook in Colab ausgeführt wird.\n",
    "Das Laden der Spacy-Modelle kann länger dauern; man kann es beschleunigen, indem man nur die Sprachen läd, die man benötigt (und die anderen Befehlszeilen löscht)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%git clone https://github.com/maria-becker/Moralization/\n",
    "%pip install HanTa\n",
    "%pip install xlsxwriter\n",
    "%pip install spacy\n",
    "\n",
    "%python -m spacy download de_core_web_lg\n",
    "%python -m spacy download en_core_web_lg\n",
    "%python -m spacy download fr_core_web_lg\n",
    "%python -m spacy download it_core_web_lg\n",
    "\n",
    "%cd \"/content/Moralization/Annotation Analysis Tools/data_analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die import-Statements müssen immer ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import label_filtering_collection as lfc\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../_utils_\")\n",
    "import corpus_extraction as ce\n",
    "import xmi_analysis_util as xau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suchfunktionen (Einzelte Korpusdateien)\n",
    "Code für die Analyse einzelner XMI-Dateien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier bitte eine Liste *file_list* erstellen, die die Dateipfade zu allen Korpusdateien enthält, die ihr analysieren wollt.\n",
    "+ *Am einfachsten erhält man einen Dateipfad, indem man die Datei im Ordersystem rechtsklickt und dann -> 'Pfad kopieren' auswählt. Und unbedingt den Pfad in Anführungszeichen setzen und davor ein kleines \"r\"!*\n",
    "+ *Wer sich ein bisschen auskennt, kann zum Erstellen der Liste auch das os-Modul benutzen, das aus diesem Grund oben importiert wurde.*\n",
    "\n",
    "Bei der Erstellung des corpus-Objekts ist zu beachten, welche Sprache im Korpus vertreten ist.\n",
    "    + Deutsch ('de')\n",
    "    + Englisch ('en')\n",
    "    + Französisch ('fr')\n",
    "    + Italienisch ('it')\n",
    "Mehrsprachliche Korpora werden nicht unterstützt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [\n",
    "    r\"/home/brunobrocai/Desktop/Code/moralization/Testfiles/test_gerichtsurteile_DE.xmi\",\n",
    "]\n",
    "corpus_collection = ce.Corpus(filepaths, 'ger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach Lexemen, die als eine bestimmte Kategorie annotiert wurden, gesucht werden.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist die Korpussammlumg. Hier einfach *corpus_collection* stehen lassen.\n",
    "+ Der zweite Parameter ist das Lexem, das gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, in der die Wortart gesucht werden soll, s.o.\n",
    "+ Der vorletzte Parameter bestimmt, ob HanTa (*True*) oder Spacy zum Lemmatisieren eingesetzt werden soll. **Achtung: HanTa eignet sich nur für Deutsch und Englisch (erzielt dort aber evtl. bessere Ergebnisse)!**\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als Excel-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: (43846, 43875)\n",
      "{'/home/brunobrocai/Desktop/Code/moralization/Testfiles/test_gerichtsurteile_DE.xmi': []}\n"
     ]
    }
   ],
   "source": [
    "word_labels = lfc.lemma_label_instances(\n",
    "    corpus_collection,\n",
    "    \"Würde\",\n",
    "    \"all_morals\",\n",
    "    hanta=True,\n",
    "    export=True\n",
    ")\n",
    "print(word_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach Wortarten, die als eine bestimmte Kategorie annotiert wurden, gesucht werden.\n",
    "Wird mit HanTa (s.u.) getaggt, nutzt die Funktion das [Stuttgart-Tübingen-Tagset](https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html). Andernfalls - also mit Spacy - wird [Universal POS tags](https://universaldependencies.org/u/pos/) benutzt, das etwas allgemeiner ist.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist die Korpussammlumg. Hier einfach *corpus_collection* stehen lassen.\n",
    "+ Der zweite Parameter ist die Wortart, die gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, in der das Lexem gesucht werden soll, s.o.\n",
    "+ Der vorletzte Parameter bestimmt, ob HanTa (*True*) oder Spacy zum Lemmatisieren eingesetzt werden soll. **Achtung: HanTa eignet sich nur für Deutsch und Englisch (erzielt dort aber evtl. bessere Ergebnisse)!**\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als Excel-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = lfc.pos_label_instances(\n",
    "    corpus_collection,\n",
    "    \"NE\",\n",
    "    \"protagonists\",\n",
    "    hanta=False,\n",
    "    export=False\n",
    ")\n",
    "lfc.print_dict(pos_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach einer Liste von Wortarten, die als eine bestimmte Kategorie annotiert wurden, gesucht werden. Dies ist vor allem daher sinnvoll, weil bestimmten Wortarten in den Tagsets mehr als ein Tag zugeordnet ist.\n",
    "Wird mit HanTa (s.u.) getaggt, nutzt die Funktion das [Stuttgart-Tübingen-Tagset](https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html). Andernfalls - also mit Spacy - wird [Universal POS tags](https://universaldependencies.org/u/pos/) benutzt, das etwas allgemeiner ist.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist die Korpussammlumg. Hier einfach *corpus_collection* stehen lassen.\n",
    "+ Der zweite Parameter ist die Wortart, die gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, in der die Wortart gesucht werden soll, s.o.\n",
    "+ Der vorletzte Parameter bestimmt, ob HanTa (*True*) oder Spacy zum Lemmatisieren eingesetzt werden soll. **Achtung: HanTa eignet sich nur für Deutsch und Englisch (erzielt dort aber evtl. bessere Ergebnisse)!**\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als Excel-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poslist_labels = lfc.poslist_label_instances_collection(\n",
    "    corpus_collection,\n",
    "    [\"NE\", \"NN\"],\n",
    "    \"protagonists\",\n",
    "    hanta=False,\n",
    "    export=False\n",
    ")\n",
    "lfc.print_dict(poslist_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach Anzahlen von Kategorie-Vergaben gesucht werden. Zum Beispiel können alle Funktionen mit zwei Protagonisten gesucht werden.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist die Korpussammlumg. Hier einfach *corpus_collection* stehen lassen.\n",
    "+ Der zweite Parameter *count* ist die Anzahl, die gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, die *count* Mal vergeben wurde\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als Excel-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_labels = lfc.count_label_instances_collection(\n",
    "    corpus_collection,\n",
    "    2,\n",
    "    \"protagonists\",\n",
    "    export=False\n",
    ")\n",
    "lfc.print_dict(count_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach spezifischen Labeln gesucht werden werden. Zum Beispiel können alle Funktionen herausgesucht werden, in denen ein Protagonist als \"Benefizient\" markiert wurde.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist die Korpussammlumg. Hier einfach *corpus_collection* stehen lassen.\n",
    "+ Der zweite Parameter ist das Label, nach dem gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, aus der das Label stammt\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als Excel-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_labels = lfc.tag_label_instances_collection(\n",
    "    corpus_collection,\n",
    "    \"Benefizient:in\",\n",
    "    \"protagonists\",\n",
    "    export=False\n",
    ")\n",
    "lfc.print_dict(tag_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
