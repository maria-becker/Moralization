{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanzen-Suche (Subkorpora)\n",
    "Mit diesem Notebook können alle Instanzen aufgelistet werden, die bestimmte Bedingungen erfüllen. Die Bedingungen können frei gewählt werden. Sie bestehen aus einer Annotationskategorie und einer weiteren Bedingung. Folgende Suchparameter gibt es:\n",
    "+ nach Wörtern (Lexemen)\n",
    "+ Nach Wortarten oder Wortartenlisten (benutzt wird das [Stuttgart-Tübingen-Tagset](https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html) oder [Universal POS tags](https://universaldependencies.org/u/pos/))\n",
    "+ Nach Anzahl vergebener Kategorietags (z.B.: 3 Moralwerte)\n",
    "+ Nach bestimmten Annotationslabels (z.B.: Forderer)\n",
    "\n",
    "Bezüglich der Annotationskategorien stehen Folgende zur Verfügung:\n",
    "+ *obj_morals*: Alle Moralwerte, die keine subjektiven Ausdrücke sind\n",
    "+ *subj_morals*: Moralwerte, die subjektive Ausdrücke sind\n",
    "+ *all_morals*: Alle Moralwerte\n",
    "+ *protagonists*: Alle Protagonisten-Token\n",
    "+ *protagonists_doubles*: Alle Protagonisten-Rollen (Achtung: manche Protagonisten-Token werden dabei doppelt gezählt - nämlich, wenn sie mehrere Rollen haben)\n",
    "+ *com_functions*: Kommunikative Funktionen\n",
    "+ *expl_demands*: Explizite Forderungen\n",
    "+ *impl_demands*: Implizite Forderungen\n",
    "+ *all_demands*: Alle Forderungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muss ausgeführt werden, wenn das Notebook in Colab ausgeführt wird.\n",
    "Das Laden der Spacy-Modelle kann länger dauern; man kann es beschleunigen, indem man nur die Sprachen läd, die man benötigt (und die anderen Befehlszeilen löscht)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!git clone https://github.com/maria-becker/Moralization/\n",
    "!pip install HanTa\n",
    "!pip install xlsxwriter\n",
    "!pip install spacy\n",
    "\n",
    "!python -m spacy download de_core_web_md\n",
    "!python -m spacy download en_core_web_md\n",
    "!python -m spacy download fr_core_web_md\n",
    "!python -m spacy download it_core_web_md\n",
    "\n",
    "%cd \"/content/Moralization/Annotation Analysis Tools/data_analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die import-Statements müssen immer ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import label_filtering_single as lf\n",
    "\n",
    "sys.path.append(\"../_utils_\")\n",
    "import corpus_extraction as ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suchfunktionen\n",
    "Code für die Analyse einzelner XMI-Dateien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier bitte den Dateipfad zur hochgeladenen XMI-Datei spezifizieren.\n",
    "*Am einfachsten die Datei im Ordersystem rechtsklicken und dann -> 'Pfad kopieren'. Und unbedingt den Pfad in Anführungszeichen setzen und davor ein kleines \"r\"!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = [\n",
    "    r\"C:\\Users\\ed304\\Documents\\Moralization\\Testfiles\\test_gerichtsurteile_DE.xmi\",\n",
    "    r\"C:\\Users\\ed304\\Documents\\Moralization\\Testfiles\\test_plenar_FR.xmi\"\n",
    "]\n",
    "\n",
    "# paramter ignore_list:\n",
    "# add labels of annotations to be ignored to the list\n",
    "corpus = ce.XMI_TO_CORPUS_OBJECT(filepath, ignore_list=[])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach Lexemen, die als eine bestimmte Kategorie annotiert wurden, gesucht werden.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist das Korpus. Hier einfach *corpus* stehen lassen.\n",
    "+ Der zweite Parameter ist das Lexem, das gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, in der die Wortart gesucht werden soll, s.o.\n",
    "+ Der vierte Parameter gibt die Sprache des Korpus an.\n",
    "    - Deutsch: \"de\"\n",
    "    - Englisch: \"en\"\n",
    "    - Italienisch: \"it\"\n",
    "    - Französisch: \"fr\"\n",
    "+ Der vorletzte Parameter bestimmt, ob HanTa (*True*) oder Spacy zum Lemmatisieren eingesetzt werden soll. **Achtung: HanTa eignet sich nur für Deutsch und Englisch (erzielt dort aber evtl. bessere Ergebnisse)!**\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als csv-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/german.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ed304/nltk_data'\n    - 'c:\\\\Users\\\\ed304\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\ed304\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ed304\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ed304\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemma_label_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSchutz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall_morals\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mde\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mhanta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mexport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ed304\\Documents\\Moralization\\Analysis Tools\\search_moralizations\\label_filtering_single.py:41\u001b[0m, in \u001b[0;36mlemma_label_instances\u001b[1;34m(corpus, lemma, label_type, language, hanta, export)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m moralization, instances \u001b[38;5;129;01min\u001b[39;00m association\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m instance \u001b[38;5;129;01min\u001b[39;00m instances:\n\u001b[1;32m---> 41\u001b[0m         tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m            \u001b[49m\u001b[43mxau\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_span\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m         tags \u001b[38;5;241m=\u001b[39m tagger\u001b[38;5;241m.\u001b[39mtag_sent(tokenized)\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags:\n",
      "File \u001b[1;32mc:\\Users\\ed304\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ed304\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\ed304\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\ed304\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\ed304\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/german.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ed304/nltk_data'\n    - 'c:\\\\Users\\\\ed304\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\ed304\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ed304\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ed304\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lf.lemma_label_instances(corpus,\n",
    "                        \"Schutz\",\n",
    "                        \"all_morals\",\n",
    "                        language=\"de\",\n",
    "                        hanta=True,\n",
    "                        export=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach Wortarten, die als eine bestimmte Kategorie annotiert wurden, gesucht werden.\n",
    "Wird mit HanTa (s.u.) getaggt, nutzt die Funktion das [Stuttgart-Tübingen-Tagset](https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html). Andernfalls - also mit Spacy - wird [Universal POS tags](https://universaldependencies.org/u/pos/) benutzt, das etwas allgemeiner ist.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist das Korpus. Hier einfach *corpus* stehen lassen.\n",
    "+ Der zweite Parameter ist die Wortart, die gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, in der das Lexem gesucht werden soll, s.o.\n",
    "+ Der vierte Parameter gibt die Sprache des Korpus an.\n",
    "    - Deutsch: \"de\"\n",
    "    - Englisch: \"en\"\n",
    "    - Italienisch: \"it\"\n",
    "    - Französisch: \"fr\"\n",
    "+ Der vorletzte Parameter bestimmt, ob HanTa (*True*) oder Spacy zum Lemmatisieren eingesetzt werden soll. **Achtung: HanTa eignet sich nur für Deutsch und Englisch (erzielt dort aber evtl. bessere Ergebnisse)!**\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als csv-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.pos_label_instances(corpus,\n",
    "\t\t\t\t\t   \"NOUN\",\n",
    "\t\t\t\t\t   \"protagonists\",\n",
    "\t\t\t\t\t   language=\"de\",\n",
    "\t\t\t\t\t   export=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach einer Liste von Wortarten, die als eine bestimmte Kategorie annotiert wurden, gesucht werden. Dies ist vor allem daher sinnvoll, weil bestimmten Wortarten in den Tagsets mehr als ein Tag zugeordnet ist.\n",
    "Wird mit HanTa (s.u.) getaggt, nutzt die Funktion das [Stuttgart-Tübingen-Tagset](https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html). Andernfalls - also mit Spacy - wird [Universal POS tags](https://universaldependencies.org/u/pos/) benutzt, das etwas allgemeiner ist.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist das Korpus. Hier einfach *corpus* stehen lassen.\n",
    "+ Der zweite Parameter ist die liste der Wortarten, die gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, in der das Lexem gesucht werden soll, s.o.\n",
    "+ Der vierte Parameter gibt die Sprache des Korpus an.\n",
    "    - Deutsch: \"de\"\n",
    "    - Englisch: \"en\"\n",
    "    - Italienisch: \"it\"\n",
    "    - Französisch: \"fr\"\n",
    "+ Der vorletzte Parameter bestimmt, ob HanTa (*True*) oder Spacy zum Lemmatisieren eingesetzt werden soll. **Achtung: HanTa eignet sich nur für Deutsch und Englisch (erzielt dort aber evtl. bessere Ergebnisse)!**\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als csv-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.poslist_label_instances(corpus,\n",
    "\t\t\t\t\t\t\t[\"NOUN\", \"PROPN\"],\n",
    "\t\t\t\t\t\t\t \"protagonists\",\n",
    "\t\t\t\t\t\t\t language=\"de\",\n",
    "\t\t\t\t\t\t\t export=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach Anzahlen von Kategorie-Vergaben gesucht werden. Zum Beispiel können alle Funktionen mit zwei Protagonisten gesucht werden.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist das Korpus. Hier einfach *corpus* stehen lassen.\n",
    "+ Der zweite Parameter *count* ist die Anzahl, die gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, die *count* Mal vergeben wurde\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als csv-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.count_label_instances(corpus,\n",
    "\t\t\t\t\t\t\t2,\n",
    "\t\t\t\t\t\t\t\"protagonists\",\n",
    "\t\t\t\t\t\t\texport=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach spezifischen Labeln gesucht werden werden. Zum Beispiel können alle Funktionen herausgesucht werden, in denen ein Protagonist als \"Benefizient\" markiert wurde.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist das Korpus. Hier einfach *corpus* stehen lassen.\n",
    "+ Der zweite Parameter ist das Label, nach dem gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, aus der das Label stammt\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als csv-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.tag_label_instances(corpus,\n",
    "\t\t\t\t\t\t\"Benefizient:in\",\n",
    "\t\t\t\t\t\t\"protagonists\",\n",
    "\t\t\t\t\t\texport=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
