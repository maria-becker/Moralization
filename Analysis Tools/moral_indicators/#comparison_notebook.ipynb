{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Moralizations with other Texts\n",
    "\n",
    "This notebook demonstrates how the modules of this directory can be used to compare linguistic features of moralizations with non-moralizing texts, such as thematizations of morality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell needs to be executed only if the notebook is run in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!git clone https://github.com/maria-becker/Moralization/\n",
    "%cd \"/content/Moralization/Annotation Analysis Tools/data_analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These modules always need to be imported; always execute the cell below.\n",
    "Note that these imports (and the imports inside the imported modules) only work with Unix/Linux-style filepaths. Under Windows, *corpus_extraction* and *xmi_analysis_util* have to be imported with other means (such as copying them into the current directory or installing them with pip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import comparison_corpus_gen as ccg\n",
    "import comparison_list_gen as clg\n",
    "import moral_vs_nonmoral as mvn\n",
    "\n",
    "import sys\n",
    "sys.path.append('../_utils_')\n",
    "import corpus_extraction as ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a List of Items to Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of creating lists of items whose frequencies we want to compare. The first is just to do it manually, like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_tokens = [\n",
    "    \"bildung\",\n",
    "    \"bildungsgerechtigkeit\",\n",
    "    \"chancengleichheit\",\n",
    "    \"erziehung\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way is to use data from the corpora.\n",
    "The module *comparison_list_gen* makes it possible to create dictionaries of lemmata or tokens that appear inside different types of annotations, where the keys are the tokens/lemmata and the values are the number of appearances.\n",
    "Subsequently, these dictionaries can be used for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moral_filepath = \"/home/bruno/Desktop/Databases/Moralization/German/Interviews-pos-SH-neu-optimiert-AW.xmi\"\n",
    "corpus = ce.CorpusData(moral_filepath)\n",
    "\n",
    "# Other functions that could be used here:\n",
    "# tokens_in_annotations() for tokens\n",
    "# pos_lemmata_in_annotations() for lemmata with specific POSs\n",
    "\n",
    "comparison_lemmata = clg.lemmata_in_annotations(\n",
    "    label_type=\"all_morals\",\n",
    "    language=\"de\",\n",
    "    corpus=corpus,\n",
    "    hanta=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the dictionary to get an overview of common lemmata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma, count in comparison_lemmata.items():\n",
    "    print(lemma, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use list comprehension to get a list of lemmata whose frequencies we can compare. In this example, we are taking all lemata with an absolute frequency of more than 10, them printing it to make sure it contains only interesting lemmata (and then remove those we do not care for)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_list = [l for l in comparison_lemmata if comparison_lemmata[l] > 10]\n",
    "print(comparison_list)\n",
    "comparison_list.remove(\"#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Corpora to Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create two more lists of strings. These should contain lists of moralizations and a type of non-moralizing texts. They will be the basis on which the frequencies of phenomena such as tokens or lemmata will be compared.\n",
    "\n",
    "This can be achieved via the *comparison_corpus_gen* module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a list of non-moralizing strings by using excel files that contain categorizations, where a 3 in the second column is assigned to moralizations and 0-2 are assigned to non-moralizing speech.\n",
    "\n",
    "In the following code, we retrieve text tagged with a 0 -- in other words, thematizations of morality (see annotation guidelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonmoral_xlsx_filepath = \"/home/bruno/Desktop/Databases/Moralization/Kategorisierungen/Alle_bearbeiteten_Annotationen_positiv_final.xlsx\"\n",
    "nonmoral_list = ccg.list_nonmoral_strings_from_xlsx(\n",
    "    nonmoral_xlsx_filepath,\n",
    "    \"Interviews\",\n",
    "    [0],\n",
    "    rm_duplicates=True\n",
    ")\n",
    "for element in nonmoral_list:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve moralizing strings via xmi files. (While it is possible to use the above function to retrieve items tagged with a 3, these annotations are of lower quality than those in the xmi files.) The function call below is rather self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moral_list = ccg.list_moralization_strings_from_xmi(\n",
    "    moral_filepath,\n",
    "    rm_duplicates=True\n",
    ")\n",
    "for element in moral_list:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the three lists of strings we generated - the lemmata whose frequencies will be compared, and the lists of moralizing and non-moralizing speech on which we are basing our analysis, to see whether the lemmata are significantly more frequent in moralizations and hence indicative of that speech act."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_dict = mvn.compare_lemma_likelihood_dict(\n",
    "    moral_list=moral_list,\n",
    "    nonmoral_list=nonmoral_list,\n",
    "    lemmata=comparison_list,\n",
    "    language='german'\n",
    ")\n",
    "for lemma, stats in comparison_dict.items():\n",
    "    print(lemma, stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write the results into an excel file, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvn.dict_to_xlsx(comparison_dict, \"output.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Subcorpora into Bigger Datasets\n",
    "The above examples uses single files for all its operations. However, to compare a genre of text, such as newspaper writing, and/or achieve higher statistical significance, we might want to use bigger corpora. In other words, we want to base our analysis on the contents of several files. This can be achieved by looping over lists of filepaths and adding lists together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this is how to create a list of moralizations out of a collection of XMIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_list = [\n",
    "    'filepath1.xmi',\n",
    "    'filepath2.xmi',\n",
    "    'filepath3.xmi',\n",
    "    'filepath4.xmi'\n",
    "]\n",
    "moral_list = []\n",
    "for filepath in filepath_list:\n",
    "    moral_subcorpus_list = ccg.list_moralization_strings_from_xmi(\n",
    "        filepath,\n",
    "        rm_duplicates=True\n",
    "    )\n",
    "    moral_list = moral_list + moral_subcorpus_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
