{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt67w0LJFhu9"
      },
      "source": [
        "# Binary Classification of Moralizing Sequences using BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaesFfsyGSlg"
      },
      "source": [
        "## Hardware\n",
        "Prepare the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7VexDadEgqR",
        "outputId": "60fbc59d-1b4f-4a43-c1e5-ae5766e21e59"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcwPfVruEl3X",
        "outputId": "4fcb04e2-0b61-4a9d-8a46-ea992cd0fdf3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu5xNqrjGrha"
      },
      "source": [
        "## Preparation\n",
        "Load data into the desired format and ready the Bert Classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1He5wnoTT33K"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AdamW, BertConfig\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import json\n",
        "import random\n",
        "\n",
        "seed_val = 1998\n",
        "random.seed(seed_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuxSWbbMT59R",
        "outputId": "041286d0-6de5-4dea-b01e-5d1fca4e656a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-german-cased',\n",
        "                                                      num_labels=2,\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpR1NomscG5G"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/full_dataset.json\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "texts = [entry[1] for entry in data]\n",
        "genres = [entry[0] for entry in data]\n",
        "labels = [entry[2] for entry in data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0rL91bYG0si"
      },
      "source": [
        "Deal with data imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fySAucHFwlo2",
        "outputId": "b1c74152-3bd5-4c80-9989-75d4ba1a2f5f"
      },
      "outputs": [],
      "source": [
        "# Check data imbalance\n",
        "print(labels.count(1)/len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H6KRxEpwI9L",
        "outputId": "0b299dbb-2752-40a0-e776-d7606a3a87ad"
      },
      "outputs": [],
      "source": [
        "# Separate data based on labels\n",
        "label_0_data = [entry for entry in data if entry[2] == 0]\n",
        "label_1_data = [entry for entry in data if entry[2] == 1]\n",
        "\n",
        "# Determine the number of samples to keep for label 0\n",
        "undersampled_label_0_data = random.sample(label_0_data, len(label_1_data))\n",
        "\n",
        "# Combine the undersampled label 0 data with the original label 1 data\n",
        "balanced_data = undersampled_label_0_data + label_1_data\n",
        "\n",
        "# Shuffle the balanced dataset to randomize the order\n",
        "random.shuffle(balanced_data)\n",
        "\n",
        "# Separate into texts, genres, and labels again\n",
        "texts = [entry[1] for entry in balanced_data]\n",
        "genres = [entry[0] for entry in balanced_data]\n",
        "labels = [entry[2] for entry in balanced_data]\n",
        "\n",
        "print(\"Balance achieved: \", labels.count(1)/len(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2FisZDeG8lz"
      },
      "source": [
        "OPTIONAL: We might want to use text-based genre labels instead of numeric ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtcAPJHcERyJ"
      },
      "outputs": [],
      "source": [
        "def get_genre_string(genre_nbr):\n",
        "    translation_dict = {\n",
        "        1: \"Nachrichten über Gerichtsurteile\",\n",
        "        2: \"Interviews in Zeitungen\",\n",
        "        3: \"Kolumnen und Kommentare in Zeitungen\",\n",
        "        4: \"Leserbriefe in Zeitungen\",\n",
        "        5: \"Plenarprotokolle\",\n",
        "        6: \"Wikipedia-Forum online\",\n",
        "        7: \"Sachbücher\"\n",
        "    }\n",
        "    return translation_dict[genre_nbr]\n",
        "\n",
        "for index, entry in enumerate(genres):\n",
        "    genres[index] = get_genre_string(entry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeDSkxPoHE5E"
      },
      "source": [
        "Tokenization. This is also where the genre labels are concatenated with the rest\n",
        "of the sequence. If this is not desired, comment out the code that does that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyLaTHHed3Df"
      },
      "outputs": [],
      "source": [
        "tokenized_texts = []\n",
        "\n",
        "for index, sents in enumerate(texts):\n",
        "    span = \" \".join(sents)\n",
        "\n",
        "    # The following line dictates whether genre labels are added to the data\n",
        "    #span = str(genres[index]) + \" [SEP] \" + span\n",
        "\n",
        "    tokenized_sents = tokenizer.encode(span, max_length=256, truncation=True, padding='max_length')\n",
        "    tokenized_texts.append(tokenized_sents)\n",
        "\n",
        "attention_masks = []\n",
        "for vector in tokenized_texts:\n",
        "    attention = [1 if token != 0 else 0 for token in vector]\n",
        "    attention_masks.append(attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYMHLUPYHlP2"
      },
      "source": [
        "Split dataset and transform into tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGPtbIpCg-WA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
        "    tokenized_texts, labels, random_state=seed_val, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(\n",
        "    attention_masks, labels, random_state=seed_val, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKpOHu1MHzdo"
      },
      "outputs": [],
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype\n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l11lEDMla2n"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbgqk3lsHpHG"
      },
      "source": [
        "Choose model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayGiWbEUmimi"
      },
      "outputs": [],
      "source": [
        "# Set Up Optimizer\n",
        "learning_rate = 3e-5\n",
        "epochs = 4\n",
        "warmup_steps = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlnV2xP6Jg8C"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import Adam\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = warmup_steps,\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSa-dV7EIaEr",
        "outputId": "3f992964-511c-4c38-947e-57fa5edf7182"
      },
      "outputs": [],
      "source": [
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_yosxIhIbpa"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sP-L6jvILqo"
      },
      "source": [
        "The following functions allows us to track the time the model is training for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWABXzNCJ24k"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfB_E4ISIhQL"
      },
      "source": [
        "Now comes the actual training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSsD3dtjJ7AF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# This training code is based on this notebook here:\n",
        "# https://colab.research.google.com/github/Ankur3107/colab_notebooks/blob/master/classification/BERT_Fine_Tuning_Sentence_Classification_v2.ipynb\n",
        "# Which in turn is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    #=================Training=======================\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically.\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        outputs = model(b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the\n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    #=================Validation=======================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = accuracy_score(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk4KmbcNIzOc"
      },
      "source": [
        "## Saving the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw3UzbUDevrZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "savefiles = \"bert_nogenre_moredata\"\n",
        "\n",
        "model.save_pretrained(savefiles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwK8r8lDz9yJ"
      },
      "outputs": [],
      "source": [
        "!zip -r bert_nogenre_moredata.zip bert_nogenre_moredata/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWLzjMmQNjv4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIbFnjmDNtqZ"
      },
      "outputs": [],
      "source": [
        "!cp bert_nogenre_moredata.zip '/content/gdrive/My Drive/'\n",
        "!ls -lt '/content/gdrive/My Drive/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmAj9co4IoYT"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxDr_SrQI_Id"
      },
      "source": [
        "Plot loss..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO26jNY9KcOg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# Plot the learning curve starting from epoch 1.\n",
        "plt.plot(range(1, len(loss_values) + 1), loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "# Set x-axis ticks and labels starting from 1.\n",
        "plt.xticks(range(1, len(loss_values) + 1))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xihm3o4MJA2V"
      },
      "source": [
        "Accuracy, precision, recall, F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgYL6LOT_O91",
        "outputId": "b1330b12-a773-4bce-98be-9ddd18d33279"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in validation_dataloader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, attention_mask, labels = batch\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        all_preds.extend(predictions)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate metrics including precision, recall, and F1-score\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1-Score: {f1}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc14OdlVJR6G"
      },
      "source": [
        "## Custom Tests\n",
        "The following code allows you to see how example sentences will be labeled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXMxMnAWhhrI"
      },
      "outputs": [],
      "source": [
        "def run_model(model, tokenizer, sentence, probability):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    tokens = tokenizer.encode(sentence, max_length=256, truncation=True, padding='max_length')\n",
        "    attention_mask = [int(token > 0) for token in tokens]\n",
        "\n",
        "    tokens = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
        "    attention_mask = torch.tensor(attention_mask).unsqueeze(0)  # Add batch dimension\n",
        "    tokens = tokens.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "    # Make the prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Get the predicted class (assuming it's a binary classification task)\n",
        "    # Apply softmax activation\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "    if probability:\n",
        "        return probabilities\n",
        "    else:\n",
        "        return torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "\n",
        "def label_sentence(model, tokenizer, sentence,\n",
        "                   genre=None, probability=False):\n",
        "\n",
        "    if genre is not None:\n",
        "        return run_model(model, tokenizer,\n",
        "                         str(genre) + \" [SEP] \" + sentence,\n",
        "                         probability)\n",
        "    else:\n",
        "        genre_ratings = []\n",
        "        for n in range(1, 8):\n",
        "            try:\n",
        "                genre = get_genre_string(n)\n",
        "            except:\n",
        "                genre = n\n",
        "            genre_ratings.append(run_model(model, tokenizer,\n",
        "                                           str(genre) + \" [SEP] \" + sentence,\n",
        "                                           probability))\n",
        "\n",
        "        return genre_ratings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK1_9iGNkPDm",
        "outputId": "01104415-cf92-4239-cd19-4dde2dec45c8"
      },
      "outputs": [],
      "source": [
        "sequence = 'Wir müssen mal.'\n",
        "result = run_model(model, tokenizer, sequence, False)\n",
        "#print(result)\n",
        "\n",
        "if result:\n",
        "    print(f\"Moralisierung\")\n",
        "else:\n",
        "    print(\"Keine Moralisierung\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSd6vIYLJfkv"
      },
      "source": [
        "You can also retrieve all error on the validation set..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9Bf3iMzgibK"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "all_results_val = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in validation_dataloader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, attention_mask, labels = batch\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "        for index, input_id in enumerate(input_ids):\n",
        "            all_results_val.append((tokenizer.decode(input_id), labels[index].item(), predictions[index]))\n",
        "\n",
        "for result in all_results_val:\n",
        "    if result[1] != result[2]:\n",
        "        print(result[1], result[2], result[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlLH-TYbJj14"
      },
      "source": [
        "or everything the model classified correctly..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIRb0bZ4EmGF"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "all_results_val = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in validation_dataloader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, attention_mask, labels = batch\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "        for index, input_id in enumerate(input_ids):\n",
        "            all_results_val.append((tokenizer.decode(input_id), labels[index].item(), predictions[index]))\n",
        "\n",
        "for result in all_results_val:\n",
        "    if result[1] == result[2]:\n",
        "        print(result[1], result[2], result[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
