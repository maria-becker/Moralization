{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanzen-Suche (Subkorpora)\n",
    "Mit diesem Notebook können alle Instanzen aufgelistet werden, die bestimmte Bedingungen erfüllen. Die Bedingungen können frei gewählt werden. Sie bestehen aus einer Annotationskategorie und einer weiteren Bedingung. Folgende Suchparameter gibt es:\n",
    "+ nach Wörtern (Lexemen)\n",
    "+ Nach Wortarten oder Wortartenlisten (benutzt wird das [Stuttgart-Tübingen-Tagset](https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html) oder [Universal POS tags](https://universaldependencies.org/u/pos/))\n",
    "+ Nach Anzahl vergebener Kategorietags (z.B.: 3 Moralwerte)\n",
    "+ Nach bestimmten Annotationslabels (z.B.: Forderer)\n",
    "\n",
    "Bezüglich der Annotationskategorien stehen Folgende zur Verfügung:\n",
    "+ *obj_morals*: Alle Moralwerte, die keine subjektiven Ausdrücke sind\n",
    "+ *subj_morals*: Moralwerte, die subjektive Ausdrücke sind\n",
    "+ *all_morals*: Alle Moralwerte\n",
    "+ *protagonists*: Alle Protagonisten-Token\n",
    "+ *protagonists_doubles*: Alle Protagonisten-Rollen (Achtung: manche Protagonisten-Token werden dabei doppelt gezählt - nämlich, wenn sie mehrere Rollen haben)\n",
    "+ *com_functions*: Kommunikative Funktionen\n",
    "+ *expl_demands*: Explizite Forderungen\n",
    "+ *impl_demands*: Implizite Forderungen\n",
    "+ *all_demands*: Alle Forderungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muss ausgeführt werden, wenn das Notebook in Colab ausgeführt wird.\n",
    "Das Laden der Spacy-Modelle kann länger dauern; man kann es beschleunigen, indem man nur die Sprachen läd, die man benötigt (und die anderen Befehlszeilen löscht)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!git clone https://github.com/maria-becker/Moralization/\n",
    "!pip install HanTa\n",
    "!pip install xlsxwriter\n",
    "!pip install spacy\n",
    "\n",
    "!python -m spacy download de_core_web_md\n",
    "!python -m spacy download en_core_web_md\n",
    "!python -m spacy download fr_core_web_md\n",
    "!python -m spacy download it_core_web_md\n",
    "\n",
    "%cd \"/content/Moralization/Annotation Analysis Tools/data_analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die import-Statements müssen immer ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import label_filtering_single as lf\n",
    "import corpus_extraction as ce\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suchfunktionen\n",
    "Code für die Analyse einzelner XMI-Dateien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier bitte den Dateipfad zur hochgeladenen XMI-Datei spezifizieren.\n",
    "*Am einfachsten die Datei im Ordersystem rechtsklicken und dann -> 'Pfad kopieren'. Und unbedingt den Pfad in Anführungszeichen setzen und davor ein kleines \"r\"!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"filepath.xmi\"\n",
    "corpus = ce.CorpusData(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach Lexemen, die als eine bestimmte Kategorie annotiert wurden, gesucht werden.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist das Korpus. Hier einfach *corpus* stehen lassen.\n",
    "+ Der zweite Parameter ist das Lexem, das gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, in der die Wortart gesucht werden soll, s.o.\n",
    "+ Der vierte Parameter gibt die Sprache des Korpus an.\n",
    "    - Deutsch: \"de\"\n",
    "    - Englisch: \"en\"\n",
    "    - Italienisch: \"it\"\n",
    "    - Französisch: \"fr\"\n",
    "+ Der vorletzte Parameter bestimmt, ob HanTa (*True*) oder Spacy zum Lemmatisieren eingesetzt werden soll. **Achtung: HanTa eignet sich nur für Deutsch und Englisch (erzielt dort aber evtl. bessere Ergebnisse)!**\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als csv-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.lemma_label_instances(corpus,\n",
    "\t\t\t\t\t\t\"signore\",\n",
    "\t\t\t\t\t\t\"protagonists\",\n",
    "\t\t\t\t\t\tlanguage=\"it\",\n",
    "                        hanta=False,\n",
    "\t\t\t\t\t\texport=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach Wortarten, die als eine bestimmte Kategorie annotiert wurden, gesucht werden.\n",
    "Wird mit HanTa (s.u.) getaggt, nutzt die Funktion das [Stuttgart-Tübingen-Tagset](https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html). Andernfalls - also mit Spacy - wird [Universal POS tags](https://universaldependencies.org/u/pos/) benutzt, das etwas allgemeiner ist.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist das Korpus. Hier einfach *corpus* stehen lassen.\n",
    "+ Der zweite Parameter ist die Wortart, die gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, in der das Lexem gesucht werden soll, s.o.\n",
    "+ Der vierte Parameter gibt die Sprache des Korpus an.\n",
    "    - Deutsch: \"de\"\n",
    "    - Englisch: \"en\"\n",
    "    - Italienisch: \"it\"\n",
    "    - Französisch: \"fr\"\n",
    "+ Der vorletzte Parameter bestimmt, ob HanTa (*True*) oder Spacy zum Lemmatisieren eingesetzt werden soll. **Achtung: HanTa eignet sich nur für Deutsch und Englisch (erzielt dort aber evtl. bessere Ergebnisse)!**\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als csv-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.pos_label_instances(corpus,\n",
    "\t\t\t\t\t   \"NOUN\",\n",
    "\t\t\t\t\t   \"protagonists\",\n",
    "\t\t\t\t\t   language=\"it\",\n",
    "\t\t\t\t\t   export=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach einer Liste von Wortarten, die als eine bestimmte Kategorie annotiert wurden, gesucht werden. Dies ist vor allem daher sinnvoll, weil bestimmten Wortarten in den Tagsets mehr als ein Tag zugeordnet ist.\n",
    "Wird mit HanTa (s.u.) getaggt, nutzt die Funktion das [Stuttgart-Tübingen-Tagset](https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html). Andernfalls - also mit Spacy - wird [Universal POS tags](https://universaldependencies.org/u/pos/) benutzt, das etwas allgemeiner ist.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist das Korpus. Hier einfach *corpus* stehen lassen.\n",
    "+ Der zweite Parameter ist die liste der Wortarten, die gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, in der das Lexem gesucht werden soll, s.o.\n",
    "+ Der vierte Parameter gibt die Sprache des Korpus an.\n",
    "    - Deutsch: \"de\"\n",
    "    - Englisch: \"en\"\n",
    "    - Italienisch: \"it\"\n",
    "    - Französisch: \"fr\"\n",
    "+ Der vorletzte Parameter bestimmt, ob HanTa (*True*) oder Spacy zum Lemmatisieren eingesetzt werden soll. **Achtung: HanTa eignet sich nur für Deutsch und Englisch (erzielt dort aber evtl. bessere Ergebnisse)!**\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als csv-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.poslist_label_instances(corpus,\n",
    "\t\t\t\t\t\t\t[\"NOUN\", \"PROPN\"],\n",
    "\t\t\t\t\t\t\t \"protagonists\",\n",
    "\t\t\t\t\t\t\t language=\"it\",\n",
    "\t\t\t\t\t\t\t export=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach Anzahlen von Kategorie-Vergaben gesucht werden. Zum Beispiel können alle Funktionen mit zwei Protagonisten gesucht werden.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist das Korpus. Hier einfach *corpus* stehen lassen.\n",
    "+ Der zweite Parameter *count* ist die Anzahl, die gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, die *count* Mal vergeben wurde\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als csv-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.count_label_instances(corpus,\n",
    "\t\t\t\t\t\t\t2,\n",
    "\t\t\t\t\t\t\t\"protagonists\",\n",
    "\t\t\t\t\t\t\texport=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach spezifischen Labeln gesucht werden werden. Zum Beispiel können alle Funktionen herausgesucht werden, in denen ein Protagonist als \"Benefizient\" markiert wurde.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist das Korpus. Hier einfach *corpus* stehen lassen.\n",
    "+ Der zweite Parameter ist das Label, nach dem gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, aus der das Label stammt\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als csv-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.tag_label_instances(corpus,\n",
    "\t\t\t\t\t\t\"Benefizient:in\",\n",
    "\t\t\t\t\t\t\"protagonists\",\n",
    "\t\t\t\t\t\texport=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
