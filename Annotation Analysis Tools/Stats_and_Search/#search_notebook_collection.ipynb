{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanzen-Suche (Korpussammlung)\n",
    "Mit diesem Notebook können alle Instanzen aufgelistet werden, die bestimmte Bedingungen erfüllen. Die Bedingungen können frei gewählt werden. Sie bestehen aus einer Annotationskategorie und einer weiteren Bedingung. Folgende Suchparameter gibt es:\n",
    "+ nach Wörtern (Lexemen)\n",
    "+ Nach Wortarten oder Wortartenlisten (benutzt wird das [Stuttgart-Tübingen-Tagset](https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html))\n",
    "+ Nach Anzahl vergebener Kategorietags (z.B.: 3 Moralwerte)\n",
    "+ Nach bestimmten Annotationslabels (z.B.: Forderer)\n",
    "\n",
    "Bezüglich der Annotationskategorien stehen Folgende zur Verfügung:\n",
    "+ *obj_morals*: Alle Moralwerte, die keine subjektiven Ausdrücke sind\n",
    "+ *subj_morals*: Moralwerte, die subjektive Ausdrücke sind\n",
    "+ *all_morals*: Alle Moralwerte\n",
    "+ *protagonists*: Alle Protagonisten-Token\n",
    "+ *protagonists_doubles*: Alle Protagonisten-Rollen (Achtung: manche Protagonisten-Token werden dabei doppelt gezählt - nämlich, wenn sie mehrere Rollen haben)\n",
    "+ *com_functions*: Kommunikative Funktionen\n",
    "+ *expl_demands*: Explizite Forderungen\n",
    "+ *impl_demands*: Implizite Forderungen\n",
    "+ *all_demands*: Alle Forderungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muss ausgeführt werden, wenn das Notebook in Colab ausgeführt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!git clone https://github.com/maria-becker/Moralization/\n",
    "!pip install HanTa\n",
    "!pip install xlsxwriter\n",
    "%cd \"/content/Moralization/Annotation Analysis Tools/data_analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die import-Statements müssen immer ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import label_filtering_collection as lfc\n",
    "import corpus_extraction as ce\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suchfunktionen (Einzelte Korpusdateien)\n",
    "Code für die Analyse einzelner XMI-Dateien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier bitte eine Liste *file_list* erstellen, die die Dateipfade zu allen Korpusdateien enthält, die ihr analysieren wollt.\n",
    "+ *Am einfachsten erhält man einen Dateipfad, indem man die Datei im Ordersystem rechtsklickt und dann -> 'Pfad kopieren' auswählt. Und unbedingt den Pfad in Anführungszeichen setzen und davor ein kleines \"r\"!*\n",
    "+ *Wer sich ein bisschen auskennt, kann zum Erstellen der Liste auch das os-Modul benutzen, das aus diesem Grund oben importiert wurde.*\n",
    "\n",
    "Bei der Erstellung des corpus-Objekts ist zu beachten, welche Sprache in eurem Korpus vertreten sind. Mehre Sprachen gleichzeitig werden nicht unterstützt!\n",
    "Folgende Sprachen stehen derzeit zur Verfügung:\n",
    "+ Deutsch ('ger')\n",
    "+ Englisch ('en')\n",
    "\n",
    "Weitere Sprachen werden folgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = r\"filepath1\"\n",
    "path2 = r\"filepath2\"\n",
    "\n",
    "filepaths = [path1,\n",
    "             path2]\n",
    "corpus_collection = ce.CorpusCollection(filepaths, 'ger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach Lexemen, die als eine bestimmte Kategorie annotiert wurden, gesucht werden.\n",
    "\n",
    "**ACHTUNG: Bis jetzt werden nur das Deutsche (\"ger\") und das Englische (\"en\") unterstützt. Weitere Sprachen werden folgen.**\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist die Korpussammlumg. Hier einfach *corpus_collection* stehen lassen.\n",
    "+ Der zweite Parameter ist das Lexem, das gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, in der die Wortart gesucht werden soll, s.o.\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als Excel-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_labels = lfc.word_label_instances_collection(corpus_collection,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"Demokratie\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"all_morals\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\texport=True)\n",
    "lfc.print_dict(word_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach Wortarten, die als eine bestimmte Kategorie annotiert wurden, gesucht werden.\n",
    "Die Funktion nutzt das [Stuttgart-Tübingen-Tagset](https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html).\n",
    "\n",
    "**ACHTUNG: Bis jetzt werden nur das Deutsche (\"ger\") und das Englische (\"en\") unterstützt. Weitere Sprachen werden folgen.**\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist die Korpussammlumg. Hier einfach *corpus_collection* stehen lassen.\n",
    "+ Der zweite Parameter ist die Wortart, die gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, in der das Lexem gesucht werden soll, s.o.\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als Excel-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = lfc.pos_label_instances_collection(corpus_collection,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"NE\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"protagonists\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\texport=False)\n",
    "lfc.print_dict(pos_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach einer Liste von Wortarten, die als eine bestimmte Kategorie annotiert wurden, gesucht werden. Diese Suche hat daher ihre eigene Funktion, weil das [Stuttgart-Tübingen-Tagset](https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html) oft größere Wortarten wie Pronomen weiter aufteilt.\n",
    "\n",
    "**ACHTUNG: Bis jetzt werden nur das Deutsche (\"ger\") und das Englische (\"en\") unterstützt. Weitere Sprachen werden folgen.**\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist die Korpussammlumg. Hier einfach *corpus_collection* stehen lassen.\n",
    "+ Der zweite Parameter ist die Wortart, die gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, in der die Wortart gesucht werden soll, s.o.\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als Excel-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poslist_labels = lfc.poslist_label_instances_collection(corpus_collection,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t[\"NE\", \"NN\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"protagonists\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\texport=False)\n",
    "lfc.print_dict(poslist_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach Anzahlen von Kategorie-Vergaben gesucht werden. Zum Beispiel können alle Funktionen mit zwei Protagonisten gesucht werden.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist die Korpussammlumg. Hier einfach *corpus_collection* stehen lassen.\n",
    "+ Der zweite Parameter *count* ist die Anzahl, die gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, die *count* Mal vergeben wurde\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als Excel-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_labels = lfc.count_label_instances_collection(corpus_collection,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t2,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"protagonists\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\texport=False)\n",
    "lfc.print_dict(count_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion kann nach spezifischen Labeln gesucht werden werden. Zum Beispiel können alle Funktionen herausgesucht werden, in denen ein Protagonist als \"Benefizient\" markiert wurde.\n",
    "\n",
    "Parameter:\n",
    "+ Der erste Parameter ist die Korpussammlumg. Hier einfach *corpus_collection* stehen lassen.\n",
    "+ Der zweite Parameter ist das Label, nach dem gesucht werden soll\n",
    "+ Der dritte Parameter ist die Kategorie, aus der das Label stammt\n",
    "+ Der letzte Parameter *export* (*True* oder *False*) gibt an, ob die Daten als Excel-Datei im derzeitigen Verzeichnis abgelegt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_labels = lfc.tag_label_instances_collection(corpus_collection,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"Benefizient:in\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"protagonists\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\texport=False)\n",
    "lfc.print_dict(tag_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
